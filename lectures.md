# Lectures

## Lecture 1: Introduction and a brief history of neural networks

This lecture maps a travel route past the three waves of neural
network hype: the Perceptron era, the Back-propagation era, and the
Deep learning era. It sketches the evolution from Jeff Elman's
recurrent neural nets to GRUs, LSTMs and the Transformer.

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vRkYxTkBF46de_1v7PZFRVNo4x6Mxdoc_7S19MNSZLVruw2ThpwOfsdb1rGOEqpOb7ZB5GU0HmAagHN/pubembed?start=false&loop=false&delayms=60000" frameborder="0" width="780" height="484" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

## Lecture 2: Transformers: Key concepts and building blocks

The Transformer is the current best solution to predicting the next
word, with autoregression and an impressive method to keep track of
all things that matter in the input seen so far, *attention*. This
lecture introduces the key concepts and building blocks of the
Transformer architecture.

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vRpKK7zSvUDG2ZvRPezXswYL8x5okovHlEFcM7fQHl63Rxat6jKx2GPHVMtoeWtrQeZJ6Yvt4V74TtI/pubembed?start=false&loop=false&delayms=60000" frameborder="0" width="780" height="484" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

## Lecture 3: Transformers recap, In-context learning, Post-training, Tools and agents

## Lecture 4: Benchmarking LLMs

## Lecture 5: Data, bias, alignment

Data is a key ingredient to training and fine-tuning
Transformers. What do we know from language data, and what does Zipf's
law predict? If commercial LLMs are trained on trillions of tokens,
where does all that data come from? What type of biases occur in them?

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vSnhPPkLkRBv4eG5etjYcJwOyfftEN1nkO5T-wr_c5Kpc17bTx_FGQMSskPgxIib1b2qsT3T9MrhGR9/pubembed?start=false&loop=false&delayms=60000" frameborder="0" width="780" height="484" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

## Lecture 6: LLM Optimization

## Lecture 7: Reasoning in LLMs

## Lecture 8: RAG and QA Applications

## Lecture 9: Miscellaneous

A Transformers course cannot be over in 8 lectures; there are so many
more interesting topics to cover. Let's talk about Mixtures of
Experts, watermarking and fingerprinting, the predictive brain in
cognitive neuroscience, and
[TESCREAL](https://en.wikipedia.org/wiki/TESCREAL).

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vSeSn4TJXrX2lW9RUV87LwywfkczSmwKOsS6minER7YNS7XylMZCCOF1fFUzh97P37un4x1ggCEzxjj/pubembed?start=false&loop=false&delayms=60000" frameborder="0" width="780" height="484" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

## Citations

You can also cite references that are stored in a `bibtex` file. For example,
the following syntax: `` {cite}`holdgraf_evidence_2014` `` will render like
this: {cite}`holdgraf_evidence_2014`.

Moreover, you can insert a bibliography into your page with this syntax:
The `{bibliography}` directive must be used for all the `{cite}` roles to
render properly.
For example, if the references for your book are stored in `references.bib`,
then the bibliography is inserted with:

```{bibliography}
```
