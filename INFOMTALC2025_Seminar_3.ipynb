{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antalvdb/antalvdb.github.io/blob/main/INFOMTALC2025_Seminar_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTONZ2jLoqtz"
      },
      "source": [
        "#Transformers: Applications in Language and Communication (INFOMTALC)\n",
        "\n",
        "## Seminar 3: The Transformer Anatomy Lesson\n",
        "\n",
        "The colab for this seminar consists of two parts:\n",
        "\n",
        "- PART I: Writing up a transformer encoder\n",
        "- PART II: Visualizing attention\n",
        "\n",
        "The overall goal is to get a better intuition about the inner workings of a transformer model. PART I is a read-and-click-through sequence of cells, while PART II contains some exercises."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART I: A sketch of transformer encoder implementation\n",
        "\n",
        "We will now implement a transformer encoder in PyTorch! Note that we will not train it. We will write out all the components and put them together, so that all the matrices and vectors have the right size and they interact in the correct way, but the actual numbers in all these vectors and components will be random (so that if we try to run the resulting module, the output will be garbage).\n",
        "\n",
        "In this implementation, we pretty closely follow Chapter 3 of the textbook. In order to dive deeper into implementational variants of different transformer blocks and to try out the actual training, we recommend you to follow Andrej Karpathy's [\"Let's build GPT\" video](https://youtu.be/kCc8FmEb1nY) -- it would be too long for us to cover in a seminar, but there are at least three reasons to watch it:\n",
        "\n",
        "1. Andrej Karpathy's educational materials are always amazing, and all his video tutorials and lectures are 100% worth watching.\n",
        "2. Here, we will implement just the transformer encoder, and Andrej Karpathy's video walks you through an implementation of the decoder, and the implementation is somewhat different in different points, it's good to see how different implementational decisions can be made.\n",
        "3. The video walks you through training the model as well, not just putting the components together.\n",
        "\n",
        "But let's walk through our implementation.\n",
        "\n",
        "Recall, during the lecture, we said that the transformer encoder consists of, first of all, a component that **embeds tokens** (that is, maps a sequence of token IDs to a sequence of vectors) and then a bunch of **encoder layers** that apply to these vectors sequentially to modify these embeddings.\n",
        "\n",
        "> ðŸ’¡ We will build our model directly in PyTorch, without using the `transformers` library by HuggingFace. However, the `transformers` library is actually only a wrapper that makes working with PyTorch (or the alternative, TensorFlow) easier, and for models that rely on PyTorch there is not much difference between a `transformers` model and a PyTorch model. In fact, a model that is retrieved using the `transformers` library is an instance of a subclass of PyTorch's `Module` class:\n",
        ">\n",
        "> ```python\n",
        "> >>> from transformers import BertModel\n",
        "> >>> from torch import nn\n",
        "> >>> model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "> >>> isinstance(model, nn.Module)\n",
        "True\n",
        "```\n",
        "\n",
        "\n",
        "### 1) Embedding\n",
        "\n",
        "Let's first write a block that produces these embeddings given the token IDs. We will use the classic BERT model (``bert-base-uncased``) as our guide to the sizes of vectors, dimensions, number of stacked components etc. We can load the configuration of the model to use as a reference:"
      ],
      "metadata": {
        "id": "28_eVSR5ecN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoConfig\n",
        "\n",
        "model_ckpt = \"bert-base-uncased\"\n",
        "config = AutoConfig.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "g68coLLCefwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's just print out the config and see what is specified there:"
      ],
      "metadata": {
        "id": "yIbC5jwBepHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "id": "aykVWNlFefyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance, since we want to define our embeddings, we need to decide on the size of the vocabulary (what is our range of token IDs is going to be?), the max sequence length (what is our range of position IDs is going to be for positional embeddings?) and the hidden size (what's the length of embedding vectors?). We will use the values from BERT:"
      ],
      "metadata": {
        "id": "RorqlAdGexWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = config.vocab_size\n",
        "hidden_size = config.hidden_size\n",
        "max_position_embeddings = config.max_position_embeddings\n",
        "\n",
        "vocab_size, hidden_size, max_position_embeddings"
      ],
      "metadata": {
        "id": "yawtl-W3ef0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here below is our Embeddings class. Let's go over what we are doing here. We will need regular token embeddings and positional embeddings (at the end, we add normalization to keep the values of the resulting embeddings under control). We use [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) as a look-up tool to retrieve an embedding vector given a token ID. Here, our embeddings (both regular token embeddings and positional embeddings) are simply random since we don't set them to any particular values when initializing.\n",
        "\n",
        "In the forward pass, we check the length of the sequence to then assign each token a position ID, and then we map each token ID to the corresponding embedding from the embeddings table, map each token to its positional embedding, simply sum up corresponding values of the two embedding vectors per token and normalize the result (``nn.LayerNorm`` subtracts the mean from each value and divides by variance, we ignore some additional details here)."
      ],
      "metadata": {
        "id": "NG-LTt9pedwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, max_position_embeddings):\n",
        "    super().__init__()\n",
        "    self.token_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
        "    self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
        "    self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    # Create position IDs for input sequence\n",
        "    seq_length = input_ids.size(1)\n",
        "    position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0) # Create token and position embeddings\n",
        "    token_embeddings = self.token_embeddings(input_ids)\n",
        "    position_embeddings = self.position_embeddings(position_ids)\n",
        "    # Combine token and position embeddings\n",
        "    embeddings = token_embeddings + position_embeddings\n",
        "    embeddings = self.layer_norm(embeddings)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "D06SSH-DfJyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can run a batch of tokenized sentences through this embedding layer and check that the output will have the right shape: it will be a tensor with dimensions ``batch size`` (= number of sentences) times ``sequence length`` times ``length of embedding vectors``.\n",
        "\n",
        "(Note a simplifying assumption here that all sequences in a batch are the same length! We are not bothering with padding all batch sequences to the same length etc)."
      ],
      "metadata": {
        "id": "yL0PXzHUfSGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb = Embeddings(vocab_size, hidden_size, max_position_embeddings)\n",
        "emb(torch.tensor([[1, 2, 5], [1, 3, 8977]])).shape"
      ],
      "metadata": {
        "id": "FQBkybVRfNZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, if we simply tokenize some sentence with the BERT tokenizer and then run our Embeddings layer over the ``input_ids`` that the tokenizer outputs, we will get the expected result:"
      ],
      "metadata": {
        "id": "OnPmRlU3fZfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "text = \"time flies like an arrow\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "inputs.input_ids.shape, emb(inputs.input_ids).shape"
      ],
      "metadata": {
        "id": "_EHiRukvfNbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> ðŸ’¡ Note that in the second line the `emb` object is called as if it were a function. That is the way to call the `forward` method of our `Embeddings` module: behind the scenes, PyTorch calls `forward` for you. The `forward` method should not be called directly.\n",
        "\n",
        "\n",
        "### 2) Multi-head attention\n",
        "\n",
        "Now that we have our embedder, let's work on the transformer encoder layers. We will need several of them stacked on top of each other. How many of them will we have? Let's check the config:"
      ],
      "metadata": {
        "id": "VkjnpvH8fi31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_hidden_layers = config.num_hidden_layers\n",
        "num_hidden_layers"
      ],
      "metadata": {
        "id": "-nIcQIO4fJ1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT uses 12, we will do the same. Let's build one encoder layer and then stack them. One encoder layer consists of a **self-attention block**, a **feed-forward block** and a bunch of normalizations and residual connections.\n",
        "\n",
        "**Self-attention**, in turn, consists of multiple attention heads. Let's again stick to what BERT does in terms of the number of attention heads per self-attention block:"
      ],
      "metadata": {
        "id": "Hh8pBGnnfnp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads = config.num_attention_heads\n",
        "num_heads"
      ],
      "metadata": {
        "id": "xOCvir4WfmzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's code one **attention head** and then put 12 of them together for a self-attention block.\n",
        "\n",
        "We will start with the very core: **scaled dot product attention**. We implement it as a function that takes query vectors, key vectors and value vectors for all tokens in the sequence. It then calculates dot products between query and key values pairwise (here in a more efficient way, as a matrix, see [``torch.bmm``](https://pytorch.org/docs/stable/generated/torch.bmm.html) documentation). Then we apply ``softmax`` to these results to get weights that range between 0 and 1 and sum to 1 per token. Then, it does weighted averaging of value vectors: each token is now assigned a vector that is a weighted average of all value vectors, but with different weights for each token."
      ],
      "metadata": {
        "id": "sUrDQIN8fuak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value):\n",
        "  dim_k = query.size(-1)\n",
        "  scores = torch.bmm(query, key.transpose(1, 2))\n",
        "  weights = F.softmax(scores, dim=-1)\n",
        "  result = torch.bmm(weights, value)\n",
        "  return result"
      ],
      "metadata": {
        "id": "qYgZp3cyfm18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now use these function on a toy example. Imagine our input sequence is 3 tokens long. Each of these tokens has a query vector, a key vector and a value vector of length 4 each. The result of applying the ``scaled_dot_product_attention`` function is going to be a sequence of 3 vectors of length 4 each:"
      ],
      "metadata": {
        "id": "hBHsKQfPf8At"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import random\n",
        "\n",
        "def rand(n):\n",
        "  return [random() for i in range(n)] # just producing vectors of len 4 with random numbers\n",
        "\n",
        "query = torch.FloatTensor([[rand(4), rand(4), rand(4),]])\n",
        "key = torch.FloatTensor([[rand(4), rand(4), rand(4)]])\n",
        "value = torch.FloatTensor([[rand(4), rand(4), rand(4)]])\n",
        "\n",
        "scaled_dot_product_attention(query, key, value)"
      ],
      "metadata": {
        "id": "LPNPNB9Bfm4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each attention head starts with projecting token embeddings to queries, keys and values. The lengths of the query, key and value vectors are not the same as the length of the embedding vector, the reason being that transformer models typically have multiple attention heads in one attention layer, and the value vectors from multiple heads are simply concatenated together before being transmitted to the next layer. So, if the embedding vectors are of size 768 and there are 12 attention heads, the query / key / value vectors are of size 64 (we concatenate 12 vectors of 64 numbers and get a vector of size 768 again). So, here is one **attention head**:"
      ],
      "metadata": {
        "id": "XjeimqXXgGXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embed_dim, head_dim):\n",
        "      super().__init__()\n",
        "      self.q = nn.Linear(embed_dim, head_dim)\n",
        "      self.k = nn.Linear(embed_dim, head_dim)\n",
        "      self.v = nn.Linear(embed_dim, head_dim)\n",
        "\n",
        "  def forward(self, hidden_state):\n",
        "    attn_outputs = scaled_dot_product_attention(self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n",
        "    return attn_outputs"
      ],
      "metadata": {
        "id": "X9d0ZwPWgDDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we put 12 attention heads together to create multi-head attention:"
      ],
      "metadata": {
        "id": "5osyaikJgNLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads):\n",
        "    super().__init__()\n",
        "    head_dim = embed_dim // num_heads\n",
        "    self.heads = nn.ModuleList([AttentionHead(embed_dim, head_dim) for _ in range(num_heads)])\n",
        "    self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self, hidden_state):\n",
        "    x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
        "    x = self.output_linear(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "KAJCKBYsgDGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check if it works!"
      ],
      "metadata": {
        "id": "PBtRzVlcgTKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_attn = MultiHeadAttention(hidden_size, num_heads)\n",
        "attn_output = multihead_attn(emb(inputs.input_ids))\n",
        "attn_output.size()"
      ],
      "metadata": {
        "id": "RTZvIlVZgU5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Feed-forward block\n",
        "\n",
        "We are done with the attention part of the encoder block! Now, let's put together the **feedforward block**, and we can then arrange these two components together with all the remaining details such as layer normalization and skip connections. The feedforward layer simply linearly projects embeddings to a bigger space and then squeezes them back to the same size as before, through a non-linear unit. Again, we use the same intermediate size as the BERT model we are using as our reference."
      ],
      "metadata": {
        "id": "SjQWGw3egTM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intermediate_size = config.intermediate_size\n",
        "print(intermediate_size)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, hidden_size, intermediate_size):\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(hidden_size, intermediate_size)\n",
        "    self.linear_2 = nn.Linear(intermediate_size, hidden_size)\n",
        "    self.gelu = nn.GELU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear_1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.linear_2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "9GZJMWcvgdYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, let's check if it works:"
      ],
      "metadata": {
        "id": "pMEEZP-igg2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ff = FeedForward(hidden_size, num_heads)\n",
        "ff_output = ff(emb(inputs.input_ids))\n",
        "ff_output.size()"
      ],
      "metadata": {
        "id": "AFSG4OLNgjRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) The encoder\n",
        "\n",
        "Now we have all the ingredients to put together a transformer encoder layer! We will include skip connection and layer normalization as well (refer to the textbook for the discussion on where to put layer normalization! there are different options):"
      ],
      "metadata": {
        "id": "EC9vQwujedyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, hidden_size, num_heads, intermediate_size):\n",
        "    super().__init__()\n",
        "    self.layer_norm_1 = nn.LayerNorm(hidden_size)\n",
        "    self.layer_norm_2 = nn.LayerNorm(hidden_size)\n",
        "    self.attention = MultiHeadAttention(hidden_size, num_heads)\n",
        "    self.feed_forward = FeedForward(hidden_size, intermediate_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Apply layer normalization and then copy input into query, key, value\n",
        "    hidden_state = self.layer_norm_1(x)\n",
        "    # Apply attention with a skip connection\n",
        "    x = x + self.attention(hidden_state)\n",
        "    # Apply feed-forward layer with a skip connection\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "PZNNlvUdgpk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we can sequentially run the embedding and one encoder layer to produce the expected result:"
      ],
      "metadata": {
        "id": "98bhUduAgub5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tr = TransformerEncoderLayer(hidden_size, num_heads, intermediate_size)\n",
        "tr_output = tr(emb(inputs.input_ids))\n",
        "tr_output.size()"
      ],
      "metadata": {
        "id": "XdJavRVbgptr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes!\n",
        "\n",
        "Finally, here is our TRANSFORMER ENCODER assembled together. First, it embeds the tokens, then it runs them through 12 stacked encoder layers, each of which first runs self-attention and then passes the tokens through a feedforward network. The output is embeddings for each token, but modified along all these intermediate steps."
      ],
      "metadata": {
        "id": "tHM9Je8Kg3mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, max_position_embeddings, num_heads, intermediate_size):\n",
        "    super().__init__()\n",
        "    self.embeddings = Embeddings(vocab_size, hidden_size, max_position_embeddings)\n",
        "    self.layers = nn.ModuleList([TransformerEncoderLayer(hidden_size, num_heads, intermediate_size) for _ in range(num_hidden_layers)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embeddings(x)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "LxAiF3gfgpwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make sure it works. It does!"
      ],
      "metadata": {
        "id": "eDiLnmuSg9Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = TransformerEncoder(vocab_size, hidden_size, max_position_embeddings, num_heads, intermediate_size)\n",
        "encoder_output = encoder(inputs.input_ids)\n",
        "encoder_output.size()"
      ],
      "metadata": {
        "id": "z8rFtKzihDrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This encoder model is not trained -- so you cannot really use it to produce meaningful text representations. But you can use it to get a clearer intuition about the building blocks of the model and how information flows between the components of the transformer. We hope it helps!\n",
        "\n",
        "## PART II: Visualizing attention"
      ],
      "metadata": {
        "id": "5J5qmWxwg_-u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdv6duE6151e"
      },
      "source": [
        "We are going to use the `bertviz` library to visualize internals of [bert-base-uncased](https://), a pretrained BERT model. We are going to input a short text into the model, activating all 12 attention layers:\n",
        "\n",
        "> *I called Ian. I got his answering machine.*\n",
        "\n",
        "In this text we find, among all types of linguistic phenomena, a co-referential relation between 'Ian' and 'his': the two words refer to the same person. Also, the combination 'answering machine' is a strong 2-word collocation. In terms of attention, one would expect that somewhere in the attention layers, there would be attention to both phenomena."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install bertviz"
      ],
      "metadata": {
        "id": "9Zwhj7yise9t",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI4Opq07pWOK"
      },
      "outputs": [],
      "source": [
        "# Load model and retrieve attention weights\n",
        "\n",
        "from bertviz import head_view, model_view\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "model_version = 'bert-base-uncased'\n",
        "model = BertModel.from_pretrained(model_version, output_attentions=True, attn_implementation=\"eager\")\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version)\n",
        "sentence_a = \"I called Ian.\"\n",
        "sentence_b = \"I got his answering machine.\"\n",
        "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt')\n",
        "input_ids = inputs['input_ids']\n",
        "token_type_ids = inputs['token_type_ids']\n",
        "attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
        "sentence_b_start = token_type_ids[0].tolist().index(1)\n",
        "input_id_list = input_ids[0].tolist() # Batch index 0\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_id_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to explore three views offered by `bertviz`: the 'head view', the 'model view', and the 'neuron view'. Although it is far from trivial to interpret what is going on in all the attention layers and their heads, in this seminar's exercises we are going to nonetheless try."
      ],
      "metadata": {
        "id": "FLt41ABMZFKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise A\n",
        "\n",
        "Explore the 'head view'. Given a selected attention layer, double clicking on the colored tiles for each attention head (selecting only the attention weights for that head), try to find strong attention weights between the token position for 'Ian' and for 'his', and between 'answering' and 'machine'. Once you find examples of both, also track them in the 'model' and 'neuron' view."
      ],
      "metadata": {
        "id": "6WVrZ2gbaeA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise B\n",
        "\n",
        "Create a different text, but with the same type of phenomenon; between a male first name and \"his\", and between the parts of a strong 2-word collocation. Are the same heads paying attention to the same phenomena again?"
      ],
      "metadata": {
        "id": "h12rzSgffWlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise C\n",
        "\n",
        "Create short texts (from just a few words to longer multi-sentence texts) with specific linguistic phenomena, such as\n",
        "\n",
        "*   Agreement over multi-word distances (\"We never seem to agree\", between \"We\" and \"agree\")\n",
        "*   Ambiguity (\"Time files like an arrow\")\n",
        "*   Grammatical errors (\"I sees a man\")\n",
        "\n",
        "And do Exercise A again.\n"
      ],
      "metadata": {
        "id": "oAuP0Bf-eXnw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDzR8ACc3hJs"
      },
      "source": [
        "# Head View\n",
        "<b>The head view visualizes attention in one or more heads from a single Transformer layer.</b> Each line shows the attention from one token (left) to another (right). Line weight reflects the attention value (ranges from 0 to 1), while line color identifies the attention head. When multiple heads are selected (indicated by the colored tiles at the top), the corresponding  visualizations are overlaid onto one another.\n",
        "\n",
        "## Usage\n",
        "ðŸ‘‰ **Hover** over any **token** on the left/right side of the visualization to filter attention from/to that token. <br/>\n",
        "ðŸ‘‰ **Double-click** on any of the **colored tiles** at the top to filter to the corresponding attention head.<br/>\n",
        "ðŸ‘‰ **Single-click** on any of the **colored tiles** to toggle selection of the corresponding attention head. <br/>\n",
        "ðŸ‘‰ **Click** on the **Layer** drop-down to change the model layer (zero-indexed).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPQKRCZ23lul"
      },
      "outputs": [],
      "source": [
        "head_view(attention, tokens, sentence_b_start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeTVYrGm33GU"
      },
      "source": [
        "# Model View\n",
        "<b>The model view provides a birds-eye view of attention throughout the entire model</b>. Each cell shows the attention weights for a particular head, indexed by layer (row) and head (column).  The lines in each cell represent the attention from one token (left) to another (right), with line weight proportional to the attention value (ranges from 0 to 1).\n",
        "\n",
        "## Usage\n",
        "ðŸ‘‰ **Click** on any **cell** for a detailed view of attention for the associated attention head (or to unselect that cell). <br/>\n",
        "ðŸ‘‰ Then **hover** over any **token** on the left side of detail view to filter the attention from that token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb_-RtcD36Oq"
      },
      "outputs": [],
      "source": [
        "model_view(attention, tokens, sentence_b_start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlAocLxq4JB-"
      },
      "source": [
        "# Neuron View\n",
        "<b>The neuron view visualizes the intermediate representations (e.g. query and key vectors) that are used to compute attention.</b> In the collapsed view (initial state), the lines show the attention from each token (left) to every other token (right). In the expanded view, the tool traces the chain of computations that produce these attention weights.\n",
        "\n",
        "## Usage\n",
        "ðŸ‘‰ **Hover** over any of the tokens on the left side of the visualization to filter attention from that token.<br/>\n",
        "ðŸ‘‰ Then **click** on the **plus** icon that is revealed when hovering. This exposes the query vectors, key vectors, and other intermediate representations used to compute the attention weights. Each color band represents a single neuron value, where color intensity indicates the magnitude and hue the sign (blue=positive, orange=negative).<br/>\n",
        "ðŸ‘‰ Once in the expanded view, **hover** over any other **token** on the left to see the associated attention computations.<br/>\n",
        "ðŸ‘‰ **Click** on the **Layer** or **Head** drop-downs to change the model layer or head (zero-indexed).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlPL6kC44MpQ"
      },
      "outputs": [],
      "source": [
        "from bertviz.transformers_neuron_view import BertModel, BertTokenizer\n",
        "from bertviz.neuron_view import show\n",
        "\n",
        "model_type = 'bert'\n",
        "model_version = 'bert-base-uncased'\n",
        "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=True)\n",
        "show(model, model_type, tokenizer, sentence_a, sentence_b, layer=4, head=3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}