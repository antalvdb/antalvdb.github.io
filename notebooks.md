# Notebooks

## Notebook 1: Basics; and how to fine-tune a text classifier.

[Run this Notebook on Colab](https://colab.research.google.com/drive/1aAvxhsVmM2-OoDn6Fs1gSY3P_xbNmDdT?usp=sharing).

This course introduces you to Transformers, a class of deep learning
neural networks based on the Transformer architecture. Although this
architecture was introduced relatively recently, in [this 2017
paper](https://https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf),
history did not start in 2017. Many of the components in and ideas
behind the Transformer have a history that harks back to the first
wave of neural network research ([McCulloch & Pitts,
1943](https://https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf);
[Rosenblatt,
1958](https://https://psycnet.apa.org/record/1959-09865-001); [Minsky
& Papert,
1969](https://https://direct.mit.edu/books/book/3132/PerceptronsAn-Introduction-to-Computational)),
as well as the second wave ([Rumelhart & McClelland,
1986](https://mitpress.mit.edu/9780262680530/parallel-distributed-processing/);
[Elman,
1990](https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1)).

Although we will focus on Transformers in this course, in this seminar
notebook we will cover some wider basics of [Hugging
Face](https://https://huggingface.co/) (ðŸ¤—), the platform for
maintaining, downloading and running Transformer models and related
architectures and tools.

In this notebook and some of the following seminar notebooks we
follow, broadly, the book [Natural Language Processing with
Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/),
by ðŸ¤— staff members Lewis Tunstall, Leandro von Werra, and Thomas
Wolf. See the [full notebook
collection](https://github.com/nlp-with-transformers/notebooks) for
this book.

## Notebook 2

## Notebook 3:

## Notebook 4:

## Notebook 5:

## Notebook 6: 

## Notebook 7: 

## Notebook 8:

